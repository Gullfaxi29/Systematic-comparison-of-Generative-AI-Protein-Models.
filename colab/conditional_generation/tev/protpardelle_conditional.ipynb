{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43926,
     "status": "ok",
     "timestamp": 1718942437425,
     "user": {
      "displayName": "Alexander Barnett",
      "userId": "00179978372066039901"
     },
     "user_tz": -600
    },
    "id": "s96xIzYAqbNT",
    "outputId": "9a229e9a-98ff-4201-fca1-5161de263a42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6760,
     "status": "ok",
     "timestamp": 1718942444174,
     "user": {
      "displayName": "Alexander Barnett",
      "userId": "00179978372066039901"
     },
     "user_tz": -600
    },
    "id": "3VW39O_5nIhj",
    "outputId": "a6d0c072-f526-4343-f08d-8bfb78c4977d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing generation metadata read in.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import re\n",
    "import torch\n",
    "\n",
    "meta_data_filepath = \"/content/drive/MyDrive/Generative_Models/conditional_generation/protpardelle_tev/generation_metadata_protpardelle_tev.csv\"\n",
    "\n",
    "if os.path.exists(meta_data_filepath):\n",
    "  all_metadata_df = pd.read_csv(meta_data_filepath)\n",
    "  print(\"Existing generation metadata read in.\")\n",
    "else:\n",
    "  all_metadata_df = pd.DataFrame()\n",
    "  #all_metadata_df.to_csv(meta_data_filepath, index=False)\n",
    "  print(\"Created generation metadata dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89504,
     "status": "ok",
     "timestamp": 1718942533672,
     "user": {
      "displayName": "Alexander Barnett",
      "userId": "00179978372066039901"
     },
     "user_tz": -600
    },
    "id": "HE03r7Li7j17",
    "outputId": "933b5d35-6175-4315-a001-0c416e9d2ff2"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install torch transformers einops tqdm wandb rotary-embedding-torch biopython scipy torchtyping dm-tree matplotlib seaborn black ipython\n",
    "git clone https://github.com/ProteinDesignLab/protpardelle\n",
    "git clone https://github.com/dauparas/ProteinMPNN.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1718942533673,
     "user": {
      "displayName": "Alexander Barnett",
      "userId": "00179978372066039901"
     },
     "user_tz": -600
    },
    "id": "xOlmB-WoosZp",
    "outputId": "52698af0-ea42-428b-c0fd-548e44523514"
   },
   "outputs": [],
   "source": [
    "%cd protpardelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1718942533673,
     "user": {
      "displayName": "Alexander Barnett",
      "userId": "00179978372066039901"
     },
     "user_tz": -600
    },
    "id": "GalXDTA2Fvpe",
    "outputId": "1e5a3c46-2fb7-4f19-fca1-6c72a134e3e5"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1718942533673,
     "user": {
      "displayName": "Alexander Barnett",
      "userId": "00179978372066039901"
     },
     "user_tz": -600
    },
    "id": "aTfAkh16Fw85"
   },
   "outputs": [],
   "source": [
    "#There has been a bug introduced in a recent protpardelle commit (issue logged), in the meantime we will just correct it\n",
    "\n",
    "with open(\"draw_samples.py\", \"r\") as f:\n",
    "  lines = f.readlines()\n",
    "\n",
    "new_lines = []\n",
    "for line in lines:\n",
    "  new_line = line.replace(\"sampling.d\", \"inference.d\")\n",
    "  new_lines.append(new_line)\n",
    "\n",
    "with open(\"draw_samples.py\", \"w\") as f:\n",
    "  f.writelines(new_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317660,
     "status": "ok",
     "timestamp": 1718942851325,
     "user": {
      "displayName": "Alexander Barnett",
      "userId": "00179978372066039901"
     },
     "user_tz": -600
    },
    "id": "9uBPV4N-YOcn",
    "outputId": "907cacd2-86ec-41c3-e8ce-b3efd6113ac7"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "batch_size = 50\n",
    "generation_command = \"python draw_samples.py --type allatom --minlen 237 --maxlen 238 --steplen 1 --perlen 50 --input_pdb /content/drive/MyDrive/Generative_Models/conditional_generation/tev_monomer.pdb --resample_idxs 0-27,33-46,51-139,152-167,179-211,221-237\"\n",
    "meta_data = {}\n",
    "meta_data['batch_id'] = str(uuid.uuid4())\n",
    "meta_data['batch_size'] = str(batch_size)\n",
    "meta_data['Timestamp'] = str(datetime.now())\n",
    "meta_data['model'] = 'protpardelle'\n",
    "meta_data['task'] = 'all_atom_pdb_generation'\n",
    "meta_data['conditions'] = 'tev (monomer) scaffolding [--resample_idxs 0-27,33-46,51-139,152-167,179-211,221-237]'\n",
    "meta_data['gpu'] = 'T4 GPU'\n",
    "start_time = time.time()\n",
    "!{generation_command}\n",
    "end_time = time.time()\n",
    "total_job_time = end_time - start_time\n",
    "meta_data['wall_time_batch'] = str(total_job_time) + \" Seconds\"\n",
    "meta_data['wall_time_task'] = str(total_job_time/batch_size) + \" Seconds (inferred)\"\n",
    "\n",
    "for filename in os.listdir(\"/content/protpardelle/samples\"):\n",
    "    if filename.endswith(\".pdb\") and \"samp\" in filename:\n",
    "      meta_data['entity_id'] = str(uuid.uuid4())\n",
    "      meta_data['output_file_name'] = \"protpardelle_tev_\" + meta_data['entity_id'] + \".pdb\"\n",
    "      metadata_entry = pd.Series(meta_data)\n",
    "      all_metadata_df = pd.concat([all_metadata_df,pd.DataFrame(metadata_entry).T], ignore_index=True)\n",
    "      cleanup_command = f\"mv /content/protpardelle/samples/{filename} /content/drive/MyDrive/Generative_Models/conditional_generation/protpardelle_tev/{meta_data['output_file_name']}\"\n",
    "      !{cleanup_command}\n",
    "all_metadata_df.to_csv(meta_data_filepath, index=False)\n",
    "print(\"Metadata saved. Cleaning up....\")\n",
    "! rm -r /content/protpardelle/samples\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNNUy/ALtuR3u8etJdztzYO",
   "gpuType": "T4",
   "mount_file_id": "1QHRNHygCKhHmxp9PEvmnebzWAj76wLFn",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
