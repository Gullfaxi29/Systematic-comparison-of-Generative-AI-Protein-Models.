{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GpT_MXATmdSh","executionInfo":{"status":"ok","timestamp":1716508920494,"user_tz":-600,"elapsed":22714,"user":{"displayName":"Alex Barnett","userId":"15265383415858973830"}},"outputId":"0e00ac2d-72f1-4337-a82a-38fea482b1f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RzhOCFNwtYab","executionInfo":{"status":"ok","timestamp":1716509068821,"user_tz":-600,"elapsed":148331,"user":{"displayName":"Alex Barnett","userId":"15265383415858973830"}},"outputId":"5b7183d2-767d-4716-841e-03b06862aed1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'progen'...\n","remote: Enumerating objects: 248, done.\u001b[K\n","remote: Counting objects: 100% (21/21), done.\u001b[K\n","remote: Compressing objects: 100% (18/18), done.\u001b[K\n","remote: Total 248 (delta 9), reused 10 (delta 3), pack-reused 227\u001b[K\n","Receiving objects: 100% (248/248), 58.62 KiB | 1.67 MiB/s, done.\n","Resolving deltas: 100% (142/142), done.\n","/content/progen/progen2\n","--2024-05-24 00:02:01--  https://storage.googleapis.com/sfr-progen-research/checkpoints/progen2-large.tar.gz\n","Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.197.207, 209.85.145.207, 142.250.125.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.197.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5137860178 (4.8G) [application/x-tar]\n","Saving to: ‘checkpoints/progen2-large.tar.gz’\n","\n","progen2-large.tar.g 100%[===================>]   4.78G  88.5MB/s    in 64s     \n","\n","2024-05-24 00:03:05 (76.4 MB/s) - ‘checkpoints/progen2-large.tar.gz’ saved [5137860178/5137860178]\n","\n","/content/progen/progen2/checkpoints\n","/content/progen/progen2\n","./\n","./config.json\n","./pytorch_model.bin\n"]}],"source":["!git clone https://github.com/salesforce/progen\n","%cd progen/progen2\n","!wget -P checkpoints/${model} https://storage.googleapis.com/sfr-progen-research/checkpoints/progen2-large.tar.gz\n","%cd ./checkpoints\n","!mkdir progen2-large\n","%cd ..\n","!tar -xvf ./checkpoints/progen2-large.tar.gz -C ./checkpoints/progen2-large/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fUwm3v80Iet"},"outputs":[],"source":["#For the sake of speed. We're going to rework the sample.py script from the package so we are not having to reload the model every generation. So we'll load the package in and redefine some code\n","setup_code = \"\"\"\n","from setuptools import setup, find_packages\n","\n","setup(\n","    name='progen2',\n","    version='1',\n","    packages=find_packages(),\n","    install_requires=[\n","    ],\n",")\n","\"\"\"\n","\n","with open('setup.py', 'w') as setup_file:\n","    setup_file.write(setup_code)\n","print(\"setup.py created successfully.\")\n","!pip install -e .\n","!pip install accelerate\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ei3MWu1S1zfW"},"outputs":[],"source":["#Non main() classes/methods from sample.py\n","import os\n","import time\n","import random\n","import argparse\n","import torch\n","from tokenizers import Tokenizer\n","from models.progen.modeling_progen import ProGenForCausalLM\n","########################################################################\n","# util\n","class print_time:\n","    def __init__(self, desc):\n","        self.desc = desc\n","\n","    def __enter__(self):\n","        print(self.desc)\n","        self.t = time.time()\n","\n","    def __exit__(self, type, value, traceback):\n","        print(f'{self.desc} took {time.time()-self.t:.02f}s')\n","\n","\n","def set_env():\n","    os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n","\n","\n","def set_seed(seed, deterministic=True):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.backends.cudnn.deterministic = deterministic\n","        torch.backends.cudnn.benchmark = not deterministic\n","########################################################################\n","# model\n","\n","def create_model(ckpt, fp16=True):\n","    if fp16:\n","        return ProGenForCausalLM.from_pretrained(ckpt, revision='float16', torch_dtype=torch.float16, low_cpu_mem_usage=True)\n","    else:\n","        return ProGenForCausalLM.from_pretrained(ckpt)\n","\n","\n","def create_tokenizer_custom(file):\n","    with open(file, 'r') as f:\n","        return Tokenizer.from_str(f.read())\n","\n","########################################################################\n","# sample\n","\n","def sample(device, model, tokenizer, context, max_length, num_return_sequences, top_p, temp, pad_token_id):\n","\n","    with torch.no_grad():\n","        input_ids = torch.tensor(tokenizer.encode(context).ids).view([1, -1]).to(device)\n","        tokens_batch = model.generate(input_ids, do_sample=True, temperature=temp, max_length=max_length, top_p=top_p, num_return_sequences=num_return_sequences, pad_token_id=pad_token_id)\n","        as_lists = lambda batch: [batch[i, ...].detach().cpu().numpy().tolist() for i in range(batch.shape[0])]\n","        return tokenizer.decode_batch(as_lists(tokens_batch))\n","\n","def truncate(sample, terminals):\n","    pos = []\n","    for terminal in terminals:\n","        find_pos = sample.find(terminal, 1)\n","        if find_pos != -1:\n","            pos.append(find_pos)\n","    if len(pos) > 0:\n","        return sample[:(min(pos)+1)]\n","    else:\n","        return sample\n","\n","def cross_entropy(logits, target, reduction='mean'):\n","    return torch.nn.functional.cross_entropy(input=logits, target=target, weight=None, size_average=None, reduce=None, reduction=reduction)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oru5kVE5yfwu","executionInfo":{"status":"ok","timestamp":1716509197401,"user_tz":-600,"elapsed":44351,"user":{"displayName":"Alex Barnett","userId":"15265383415858973830"}},"outputId":"f380df61-ad16-448c-ea17-28b2e28ae8f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading parameters\n","loading parameters took 42.76s\n","loading tokenizer\n","loading tokenizer took 0.04s\n","sanity cross-entropy\n","1.8 1.8524295091629028 0.05242950916290279\n","sanity cross-entropy took 1.40s\n"]}],"source":["import random\n","#Pre-sampling part of Main method reworked\n","#####\n","# (0) constants\n","models_151M = [ 'progen2-small' ]\n","models_754M = [ 'progen2-medium', 'progen2-oas', 'progen2-base' ]\n","models_2B = [ 'progen2-large', 'progen2-BFD90' ]\n","models_6B = [ 'progen2-xlarge' ]\n","models = models_151M + models_754M + models_2B + models_6B\n","\n","# (1) params\n","use_model = 'progen2-large'\n","use_device='cuda:0'\n","rng_seed = random.randint(1,9999999) #Need to run 1 sample at a time to record the time per length. This also means we must use a different seed each time for the script\n","rng_deterministic = False\n","fp16 = True\n","sanity=True\n","\n","# (2) preamble\n","set_env()\n","set_seed(rng_seed, deterministic=rng_deterministic)\n","if not torch.cuda.is_available():\n","    print('falling back to cpu')\n","    args.device = 'cpu'\n","device = torch.device(use_device)\n","ckpt = f'./checkpoints/{use_model}'\n","if device.type == 'cpu':\n","    print('falling back to fp32')\n","    fp16 = False\n","# (3) load\n","with print_time('loading parameters'):\n","    model = create_model(ckpt=ckpt, fp16=fp16).to(device)\n","with print_time('loading tokenizer'):\n","    tokenizer = create_tokenizer_custom(file='tokenizer.json')\n","# (4) sanity\n","if sanity:\n","    with print_time('sanity cross-entropy'):\n","        def ce(tokens):\n","            with torch.no_grad():\n","                with torch.cuda.amp.autocast(enabled=fp16):\n","                    target = torch.tensor(tokenizer.encode(tokens).ids).to(device)\n","                    logits = model(target, labels=target).logits\n","                    # shift\n","                    logits = logits[:-1, ...]\n","                    target = target[1:]\n","                    return cross_entropy(logits=logits, target=target).item()\n","        x_uniref90bfd30 = '2GFLPFRGADEGLAAREAATLAARGTAARAYREDSWAVPVPRGLLGDLTARVAALGAASPPPADPLAVTLDLHHVTAEVALTTVLDAATLVHGQTRVLSAEDAAEAATAAAAATEAYLERLQDFVLFMSASVRVWRRGNAAGATGPEWDQWYTVADRDALGSAPTHLAVLGRQADALCHFVLDRVAWGTCGTPLWSGDEDLGNVVATFAGYADRLATAPRDLIM1'\n","        x_oas = '1EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYAMHWVRQAPWKGLEYVSAISSNGGSTYYANSVKGRFTISRDNSKNTLYLQMGSLRAEDMAVYYCARDESGYSYGWGYYFDYWGQGTLVTVSS2'\n","        x_bfd90 = '1TAPRSTRASGSEGSRPPGIPAKGRRCLPSRAGSVTPRFRHARQGTATVAKEQGRKLIASNRKARHDYHIEDTFEAGLVLTGTEVKSLRMGRASLIDGYAVFYGEELWLEGVHIPEYLNGNWTNHTPRRRRKLLLNRSELTKLAHKTSESGHTIVPLALYFKDGRAKVEIAVAKGKKAYDKRHALRERQDQREV2'\n","        checkpoint_x_ce = {\n","            'progen2-small': (x_uniref90bfd30, 2.4),\n","            'progen2-medium': (x_uniref90bfd30, 1.9),\n","            'progen2-base': (x_uniref90bfd30, 1.9),\n","            'progen2-large': (x_uniref90bfd30, 1.8),\n","            'progen2-xlarge': (x_uniref90bfd30, 1.0),\n","            'progen2-oas': (x_oas, 0.3),\n","            'progen2-BFD90': (x_bfd90, 1.3),\n","        }\n","        ce_eval = ce(checkpoint_x_ce[use_model][0])\n","        ce_target = checkpoint_x_ce[use_model][1]\n","        print(ce_target, ce_eval, abs(ce_eval - ce_target))\n","        assert abs(ce_eval - ce_target) < 0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0w9fSubMpLO2","executionInfo":{"status":"ok","timestamp":1716509199413,"user_tz":-600,"elapsed":2028,"user":{"displayName":"Alex Barnett","userId":"15265383415858973830"}},"outputId":"55fc728a-1b18-4c1f-90e4-42ac3c1f2363"},"outputs":[{"output_type":"stream","name":"stdout","text":["Existing generation metadata read in.\n","Loaded length distribution from drive\n"]}],"source":["import os\n","import shutil\n","import glob\n","import json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import uuid\n","from datetime import datetime\n","import re\n","import torch\n","import time\n","\n","meta_data_filepath = \"/content/drive/MyDrive/Generative_Models/unconditional_generation/progen2_unconditional/generation_metadata_progen2.csv\"\n","\n","if os.path.exists(meta_data_filepath):\n","  all_metadata_df = pd.read_csv(meta_data_filepath)\n","  print(\"Existing generation metadata read in.\")\n","else:\n","  all_metadata_df = pd.DataFrame()\n","  #all_metadata_df.to_csv(meta_data_filepath, index=False)\n","  print(\"Created generation metadata dataframe\")\n","\n","\n","len_dist_filepath = \"/content/drive/MyDrive/Generative_Models/unconditional_generation/progen2_unconditional/uniref50_length_dist_progen2.json\"\n","\n","if os.path.exists(len_dist_filepath):\n","  with open(len_dist_filepath, \"r\") as f:\n","    uniprot_length_dist =  json.load(f)\n","  print(\"Loaded length distribution from drive\")\n","else:\n","\n","  #https://www.uniprot.org/uniprotkb/statistics#sequence-size\n","  bins = np.array([13,51,101,151,201,251,301,351,401,451,501,551,601,651,701,751,801,851,901,951,1001,1101,1201,1301,1401,1501,1601,1701,1801,1901,2001,2101,2201,2301,2401,2501,34350])\n","  swissprot_reviewed = np.array([0,9968,43534,59796,59574,58452,52413,52846,45901,37706,30572,22287,15830,13156,9403,7870,5700,4889,5301,4109,3007,4124,2897,2207,2070,1675,834,642,587,503,395,272,386,340,234,195,1462])\n","  TrEMBL_unreviewed = np.array([0,2668805,19825275,24705701,23838128,23462438,23225451,21389271,16814580,14287105,11501843,8283150,6266068,4715059,3755005,3186452,2687314,2166878,1843669,1457871,1153537,1975953,1398765,961048,664766,517536,390552,300984,236895,210921,180246,138808,122833,102865,82441,71548,527646])\n","\n","  ecdf = np.cumsum(swissprot_reviewed) / np.sum(swissprot_reviewed)\n","  #shortest protein in uniprot is 14 res, longest is 34350 res.\n","  x = np.arange(14, 34350+1)\n","  ecdf = np.interp(x, bins, ecdf)\n","\n","  # Sample from the empirical CDF\n","  num_samples = 11000\n","  random_values = np.random.rand(num_samples)\n","  sampled_lengths = np.round(np.interp(random_values, ecdf, x)).astype(int)\n","  #ten thousand sequences up to 1000 res in length\n","  sampled_lengths = sampled_lengths[sampled_lengths <= 1000][0:10000]\n","\n","  # Plot the histogram of sampled values\n","  hist_values, bin_edges, patches = plt.hist(sampled_lengths, bins=x[0:1001-13], alpha=0.7, label='Sampled Values')\n","  plt.xlabel('X-axis label')\n","  plt.ylabel('Frequency')\n","  plt.legend()\n","  plt.show()\n","\n","  uniprot_length_dist = list(zip([int(edge) for edge in bin_edges],[int(value) for value in hist_values]))\n","  with open(len_dist_filepath, \"w\") as f:\n","      json.dump(uniprot_length_dist, f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wm7gLynf4b_L","colab":{"base_uri":"https://localhost:8080/","height":836},"executionInfo":{"status":"error","timestamp":1716509472131,"user_tz":-600,"elapsed":10522,"user":{"displayName":"Alex Barnett","userId":"15265383415858973830"}},"outputId":"4af28b9f-fe0a-4019-d56b-3d7829968691"},"outputs":[{"output_type":"stream","name":"stdout","text":["Max sampling length: 18\n","MPRLTRSVAFADFVVEGP\n","Sampled length: 18\n","saved to metadata 2024-05-24 00:11:03.571860\n","Max sampling length: 16\n","MTFDGRSAPTVVEPAV\n","Sampled length: 16\n","saved to metadata 2024-05-24 00:11:04.621323\n","Max sampling length: 15\n","MRDLDKRGMLERTAE\n","Sampled length: 15\n","saved to metadata 2024-05-24 00:11:05.627964\n","Max sampling length: 15\n","MRDAEELFLVANASE\n","Sampled length: 15\n","saved to metadata 2024-05-24 00:11:06.621454\n","Max sampling length: 15\n","MRGVVTNLTPYDAPR\n","Sampled length: 15\n","saved to metadata 2024-05-24 00:11:07.620490\n","Max sampling length: 15\n","MGLLSQAAAAAEGKQ\n","Sampled length: 15\n","saved to metadata 2024-05-24 00:11:08.607869\n","Max sampling length: 15\n","MGIRRSPTAIIAAVE\n","Sampled length: 15\n","saved to metadata 2024-05-24 00:11:09.614635\n","Max sampling length: 14\n","MDATQLGAAPEQFL\n","Sampled length: 14\n","saved to metadata 2024-05-24 00:11:10.546305\n","Max sampling length: 14\n","MALDIGSLNLAGTI\n","Sampled length: 14\n","saved to metadata 2024-05-24 00:11:11.796287\n"]},{"output_type":"error","ename":"ValueError","evalue":"max() arg is an empty sequence","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-ca5b49a0be76>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0msampling_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msampling_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampling_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m#max_length = 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Max sampling length: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"]}],"source":["\n","#sampling\n","#########\n","context = 'M'\n","#temperature and top_p are taken from example on github page rather than command arg defaults as it seems to generate better results\n","temp = 0.8\n","top_p = 0.9\n","meta_data = {}\n","i = 0\n","while i < 10000:\n","  torch.cuda.empty_cache()\n","  if all_metadata_df.empty:\n","    max_length = max([l[0] for l in uniprot_length_dist])\n","  else:\n","    sampling_lengths = {s: n for s, n in uniprot_length_dist if n > 0}\n","    for l in all_metadata_df[\"conditions\"].str.extract(r'length = (\\d+)', expand=False).astype(int):\n","      if l < 14: continue\n","      if l > 1000: continue\n","      sampling_lengths[l] = sampling_lengths[l] -1\n","    sampling_lengths = {s: n for s, n in sampling_lengths.items() if n > 0}\n","    max_length = max(sampling_lengths.keys())\n","    #max_length = 100\n","  print(\"Max sampling length: \" + str(max_length))\n","  meta_data['entity_id'] = str(uuid.uuid4())\n","  meta_data[\"batch_id\"] = None\n","  meta_data[\"batch_size\"] = None\n","  meta_data['output_file_name'] = None\n","  meta_data[\"Timestamp\"] = str(datetime.now())\n","  meta_data['model'] = 'ProGen2'\n","  meta_data['task'] = 'sequence_generation'\n","  meta_data['wall_time_batch'] = None\n","  meta_data['gpu'] = 'T4 GPU'\n","\n","  start_time = time.time()\n","  completions = sample(device=device, model=model, tokenizer=tokenizer, context=context, pad_token_id=tokenizer.encode('<|pad|>').ids[0], num_return_sequences=1, temp=temp, top_p=top_p, max_length=max_length)\n","  truncations = [truncate(completion, terminals=['1', '2']) for completion in completions]\n","  end_time = time.time()\n","  meta_data['wall_time_task'] = str(end_time-start_time) + \" Seconds\"\n","  #assuming only one sequence generated!!!\n","  sequence = truncations[0]\n","  print(sequence)\n","\n","  length = len(sequence)\n","  print(\"Sampled length: \" + str(length))\n","  meta_data['conditions'] = 'length = ' + str(length)\n","  meta_data['generated_sequence'] = sequence\n","  metadata_entry = pd.Series(meta_data)\n","  all_metadata_df = pd.concat([all_metadata_df,pd.DataFrame(metadata_entry).T], ignore_index=True)\n","\n","  i = i + 1\n","  if i % 1 == 0:\n","    all_metadata_df.to_csv(meta_data_filepath, index=False)\n","    print(\"saved to metadata \" + str(datetime.now()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Mb_LTYLTDkUm"},"outputs":[],"source":["!kill -9 -1"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}